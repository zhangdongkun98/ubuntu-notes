# - DO NOT EDIT - FILE IS AUTOGENERATED

# NOTE: The docker:stable-dind service is not used here because --add-runtime=nvidia does not work from DIND and we need the
#       runtime to perform tests

# Important gitlab-runner considerations
#
# Docker buildx is used for multi-arch images builds. This feature is currently experimental and
# must be explicitly enabled on the docker daemon performing the image builds.
#
# To run multi-arch images on x86_64, qemu-user-static and systemd are required with the following configuration:
#
# $ cat /etc/binfmt.d/qemu-static.conf
# :qemu-aarch64:M::\x7fELF\x02\x01\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x02\x00\xb7:\xff\xff\xff\xff\xff\xff\xff\x00\xff\xff\xff\xff\xff\xff\xff\xff\xfe\xff\xff:/usr/bin/qemu-aarch64-static:CF
# :qemu-ppc64le:M::\x7fELF\x02\x01\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x02\x00\x15\x00:\xff\xff\xff\xff\xff\xff\xff\x00\xff\xff\xff\xff\xff\xff\xff\xff\xfe\xff\xff\x00:/usr/bin/qemu-ppc64le-static:CF
#
# < reboot or start systemd-binfmt.service >
#
# Check with:
#
# $ systemctl status systemd-binfmt.service
# < service should be started >
#
# $ docker run -it nvidia/cuda-ppc64le:11.0-base-ubuntu18.04-rc
# < container should run>

variables:
  # Need a value of two here for checking the manifest in the last commit
  GIT_DEPTH: "2"
  # If set to "true" this will force the prepare stage to run to build the builder images that contains docker buildx and python dependencies
  REBUILD_BUILDER: "false"
  {# NO_PUSH: "true" #}
  {# NO_SCAN: "true" # Only use in development #}
  {# NO_TEST: "true" # Only use in development #}

before_script:
  - source util.sh
  - if [[ ! -z ${RELEASE_LABEL} ]]; then
        export CUDA_VERSION=${RELEASE_LABEL};
    fi
  - 'echo "TRIGGER: ${KITMAKER}"'
  - 'echo "KITMAKER: ${KITMAKER}"'
  - 'echo "ARCH: $ARCH"'
  - 'echo "IMAGE_NAME: $IMAGE_NAME"'
  - 'echo "OS: $OS"'
  - 'echo "OS_VERSION: $OS_VERSION"'
  - 'echo "OS_NAME: $OS_NAME"'
  - 'echo "CUDA_VERSION: $CUDA_VERSION"'
  - 'echo "RELEASE_LABEL: $RELEASE_LABEL"' # used by kitmaker and cuda versions >= 11.2
  - 'echo "CUDNN_VERSION: $CUDNN_VERSION"'
  - 'echo "IMAGE_TAG_SUFFIX: $IMAGE_TAG_SUFFIX"'
  - 'echo "NO_OS_SUFFIX: $NO_OS_SUFFIX"'
  # Gitlab is used to stage images
  - docker login -u "gitlab-ci-token" -p $CI_JOB_TOKEN gitlab-master.nvidia.com:5005
  # Login to NGC to pull images
  - docker login -u '$oauthtoken' -p $NVCR_TOKEN nvcr.io

stages:
  - prepare
  - trigger
  - cuda
  - cudnn
  - test
  - scan
  - deploy
  - cleanup_success
  - cleanup_failed

# builds the gitlab-cuda-builder image
prepare:
  image: docker:stable
  stage: prepare
  variables:
{% set gitlab_builder_image = "gitlab-master.nvidia.com:5005/cuda-installer/cuda/gitlab-cuda-builder" %}
    IMAGE_NAME: "{{ gitlab_builder_image }}"
  script:
    - docker build -t {{ gitlab_builder_image }} --cache-from {{ gitlab_builder_image }} .
    - docker push {{ gitlab_builder_image }}
  tags:
    - docker
  rules:
      - if: '$REBUILD_BUILDER == "true"'

.tags_template: &tags_definition
  tags:
    - docker

.tags_template_multiarch: &tags_definition_multiarch
  tags:
    - docker-multi-arch

# Only used for CUDA 8.0, will be removed once CUDA 8 support is dropped
.cuda_template_depricated: &cuda_definition_deprecated
  image: {{ gitlab_builder_image }}
  stage: cuda
  retry: 2
  script:
    - if [[ "${NO_OS_SUFFIX}" == "true" ]]; then
        export TAG_RUNTIME=" -t ${IMAGE_NAME}:${CUDA_VERSION}-runtime ";
        export TAG_DEVEL=" -t ${IMAGE_NAME}:${CUDA_VERSION}-devel ";
        export CUDA_TAGS="${CUDA_VERSION}-runtime ${CUDA_VERSION}-devel ";
      fi
    - if [[ -z $DIST_BASE_PATH ]]; then
        export DIST_BASE_PATH="dist/${OS}/${CUDA_VERSION}/${ARCH}";
      else
        export DIST_BASE_PATH="${DIST_BASE_PATH}/${OS}-${ARCH}";
      fi
    - docker buildx create --use --platform linux/${ARCH} --driver-opt image=moby/buildkit:v0.8.1 --name cuda-${ARCH}
    - docker buildx build --pull --push --platform linux/${ARCH} -t "${IMAGE_NAME}:${CUDA_VERSION}-runtime-${OS}" ${TAG_RUNTIME} "${DIST_BASE_PATH}/runtime"
                          --build-arg BUILDKIT_INLINE_CACHE=1 --progress plain
                          --cache-from=type=registry,ref=${IMAGE_NAME}:${CUDA_VERSION}-runtime-${OS}
    - docker buildx build --pull --push --platform linux/${ARCH} -t "${IMAGE_NAME}:${CUDA_VERSION}-devel-${OS}" ${TAG_DEVEL} "${DIST_BASE_PATH}/devel"
                          --build-arg "IMAGE_NAME=${IMAGE_NAME}"
                          --build-arg BUILDKIT_INLINE_CACHE=1 --progress plain
                          --cache-from=type=registry,ref=${IMAGE_NAME}:${CUDA_VERSION}-devel-${OS}
    - 'echo "CUDA_TAGS=\"${CUDA_TAGS} ${CUDA_VERSION}-runtime-${OS} ${CUDA_VERSION}-devel-${OS}\"" >> tags.env'
    - cat tags.env
  artifacts:
    reports:
      dotenv: tags.env

.cuda_base_template: &cuda_base_definition
  image: {{ gitlab_builder_image }}
  stage: cuda
  retry: 2
  script:
    - if [[ "${NO_OS_SUFFIX}" == "true" ]]; then
        export TAG_BASE=" -t ${IMAGE_NAME}:${CUDA_VERSION}-base${IMAGE_TAG_SUFFIX} ";
        export TAG_RUNTIME=" -t ${IMAGE_NAME}:${CUDA_VERSION}-runtime${IMAGE_TAG_SUFFIX} ";
        export TAG_DEVEL=" -t ${IMAGE_NAME}:${CUDA_VERSION}-devel${IMAGE_TAG_SUFFIX} ";
        export CUDA_TAGS="${CUDA_VERSION}-base${IMAGE_TAG_SUFFIX} ${CUDA_VERSION}-runtime${IMAGE_TAG_SUFFIX} ${CUDA_VERSION}-devel${IMAGE_TAG_SUFFIX} ";
      fi
    - if [[ -z $DIST_BASE_PATH ]]; then
        export DIST_BASE_PATH="dist/${OS}/${CUDA_VERSION}/${ARCH}";
      else
        export DIST_BASE_PATH="${DIST_BASE_PATH}/${OS}-${ARCH}";
      fi
    - |
      if [ ! -z $KITMAKER ] && [ ! -z $TRIGGER ]; then
          export DIST_BASE_PATH="kitpick"
          rm -rf kitpick || true
          mkdir -p kitpick || true
          find kitpick/
          NEW_CANDIDATE_URL="${CANDIDATE_URL}"
          [[ "${CANDIDATE_URL}" != */ ]] && NEW_CANDIDATE_URL="${CANDIDATE_URL}/"
          retry 3 10 time wget --recursive --directory-prefix="rekt" --reject="index.html*" --no-parent ${NEW_CANDIDATE_URL}
          cp -R rekt/${CANDIDATE_URL#"http://"}/{base,devel,runtime} kitpick/
          echo "Contents of kitpick/"
          find kitpick/
      fi
    - 'echo "DIST_BASE_PATH: ${DIST_BASE_PATH}"'
    - 'echo "TAG_BASE: ${TAG_BASE}"'
    - 'echo "TAG_RUNTIME: ${TAG_RUNTIME}"'
    - 'echo "TAG_DEVEL: ${TAG_DEVEL}"'
    # --driver-opt flag is needed to workaround https://github.com/docker/buildx/issues/386
    - docker buildx create --use --platform linux/${ARCH} --driver-opt image=moby/buildkit:v0.8.1 --name cuda-${ARCH}
    - docker buildx build --pull --push --platform linux/${ARCH} -t "${IMAGE_NAME}:${CUDA_VERSION}-base-${OS}${IMAGE_TAG_SUFFIX}" ${TAG_BASE} "${DIST_BASE_PATH}/base"
                          --build-arg BUILDKIT_INLINE_CACHE=1 --progress plain
                          --cache-from=type=registry,ref=${IMAGE_NAME}:${CUDA_VERSION}-base-${OS}${IMAGE_TAG_SUFFIX}
    - docker buildx build --pull --push --platform linux/${ARCH} -t "${IMAGE_NAME}:${CUDA_VERSION}-runtime-${OS}${IMAGE_TAG_SUFFIX}" ${TAG_RUNTIME}
                          --build-arg "IMAGE_NAME=${IMAGE_NAME}" "${DIST_BASE_PATH}/runtime"
                          --build-arg BUILDKIT_INLINE_CACHE=1 --progress plain
                          --cache-from=type=registry,ref=${IMAGE_NAME}:${CUDA_VERSION}-runtime-${OS}${IMAGE_TAG_SUFFIX}
    - docker buildx build --pull --push --platform linux/${ARCH} -t "${IMAGE_NAME}:${CUDA_VERSION}-devel-${OS}${IMAGE_TAG_SUFFIX}" ${TAG_DEVEL}
                          --build-arg "IMAGE_NAME=${IMAGE_NAME}" "${DIST_BASE_PATH}/devel"
                          --build-arg BUILDKIT_INLINE_CACHE=1 --progress plain
                          --cache-from=type=registry,ref=${IMAGE_NAME}:${CUDA_VERSION}-devel-${OS}${IMAGE_TAG_SUFFIX}
    - 'echo "CUDA_TAGS=\"${CUDA_TAGS} ${CUDA_VERSION}-base-${OS}${IMAGE_TAG_SUFFIX} ${CUDA_VERSION}-runtime-${OS}${IMAGE_TAG_SUFFIX} ${CUDA_VERSION}-devel-${OS}${IMAGE_TAG_SUFFIX}\"" >> tags.env'
    - cat tags.env
  artifacts:
    reports:
      dotenv: tags.env

.cudnn_template: &cudnn_definition
  image: {{ gitlab_builder_image }}
  stage: cudnn
  retry: 2
  script:
    - if [[ -z $DIST_BASE_PATH ]]; then
        DIST_BASE_PATH="dist/${OS}/${CUDA_VERSION}/${ARCH}";
      else
        DIST_BASE_PATH="${DIST_BASE_PATH}/${OS}-${ARCH}";
      fi
    - 'echo "DIST_BASE_PATH: ${DIST_BASE_PATH}"'
    - if [[ "${NO_OS_SUFFIX}" == "true" ]]; then
        TAG_CUDNN_RUNTIME=" -t ${IMAGE_NAME}:${CUDA_VERSION}-${CUDNN_VERSION}-runtime${IMAGE_TAG_SUFFIX} ";
        TAG_CUDNN_DEVEL=" -t ${IMAGE_NAME}:${CUDA_VERSION}-${CUDNN_VERSION}-devel${IMAGE_TAG_SUFFIX} ";
        export CUDA_EXTRA_TAGS="${CUDA_EXTRA_TAGS} ${CUDA_VERSION}-${CUDNN_VERSION}-runtime${IMAGE_TAG_SUFFIX} ${CUDA_VERSION}-${CUDNN_VERSION}-devel${IMAGE_TAG_SUFFIX}";
      fi
    - docker buildx create --use --platform linux/${ARCH} --driver-opt image=moby/buildkit:v0.8.1 --name cuda-${ARCH}
    - docker buildx build --pull --push --platform linux/${ARCH} -t "${IMAGE_NAME}:${CUDA_VERSION}-${CUDNN_VERSION}-runtime-${OS}${IMAGE_TAG_SUFFIX}" ${TAG_CUDNN_RUNTIME}
                  --build-arg "IMAGE_NAME=${IMAGE_NAME}" "${DIST_BASE_PATH}/runtime/${CUDNN_VERSION}"
                  --build-arg BUILDKIT_INLINE_CACHE=1 --progress plain
                  --cache-from=type=registry,ref=${IMAGE_NAME}:${CUDA_VERSION}-${CUDNN_VERSION}-runtime-${OS}${IMAGE_TAG_SUFFIX}
    - docker buildx build --pull --push --platform linux/${ARCH} -t "${IMAGE_NAME}:${CUDA_VERSION}-${CUDNN_VERSION}-devel-${OS}${IMAGE_TAG_SUFFIX}" ${TAG_CUDNN_DEVEL}
                  --build-arg "IMAGE_NAME=${IMAGE_NAME}" "${DIST_BASE_PATH}/devel/${CUDNN_VERSION}"
                  --build-arg BUILDKIT_INLINE_CACHE=1 --progress plain
                  --cache-from=type=registry,ref=${IMAGE_NAME}:${CUDA_VERSION}-${CUDNN_VERSION}-devel-${OS}${IMAGE_TAG_SUFFIX}
    # this is not a great way to pass information between stages, but it is currently the only way
    - 'echo "CUDA_TAGS=${CUDA_TAGS}" > ${CUDNN_VERSION}finaltags.env'
    - 'echo "CUDA_${CUDNN_VERSION}_TAGS=\"${CUDA_VERSION}-${CUDNN_VERSION}-devel-${OS}${IMAGE_TAG_SUFFIX} ${CUDA_VERSION}-${CUDNN_VERSION}-runtime-${OS}${IMAGE_TAG_SUFFIX}\"" >> ${CUDNN_VERSION}finaltags.env'
    - 'echo "CUDA_EXTRA_${CUDNN_VERSION}_TAGS=\"${CUDA_EXTRA_TAGS}\"" >> ${CUDNN_VERSION}finaltags.env'
    - cat ${CUDNN_VERSION}finaltags.env

.test_template: &test_definition
  image: {{ gitlab_builder_image }}
  stage: test
  tags:
    - cuda-test
  script:
    - |
      bash -e ./test/scripts/bats_install.sh
      bash -e ./test/scripts/run_tests.sh

.scan_template: &scan_definition
  image: {{ gitlab_builder_image }}
  stage: scan
  <<: *tags_definition
  script:
    - |
      git clone https://gitlab-ci-token:${CI_JOB_TOKEN}@gitlab-master.nvidia.com/sectooling/scanning/contamer.git
      cd contamer
      pip3 install -r requirements.txt
      docker pull ${IMAGE_NAME}:${CUDA_VERSION}-devel-${OS}${IMAGE_TAG_SUFFIX}

      # CVE-2020-14352 CVE-2020-15888: librepo, lua-libs packages WAIVED via https://nvbugswb.nvidia.com/NvBugs5/SWBug.aspx?bugid=3028123&cmtNo=35
      # CVE-2019-25013 WAIVED via https://nvbugs/3229249
      python3 contamer.py --debug-logs -ls --fail-on-non-os ${IMAGE_NAME}:${CUDA_VERSION}-devel-${OS}${IMAGE_TAG_SUFFIX} \
          --suppress-vulns CVE-2020-14352 CVE-2020-15888 CVE-2019-25013 | tee /tmp/output
      retval=$?
      echo "Contamer return value: ${retval}";

      docker rmi -f ${IMAGE_NAME}:${CUDA_VERSION}-devel-${OS}${IMAGE_TAG_SUFFIX}
      if [[ $retval -ne 0 ]]; then
          exit 1;
      else
          # we need to look at the output for "Traceback" because contamer doesn't return non-zero on exception failure...
          if cat /tmp/output | grep -q "Traceback\|Exception: Image analysis failed"; then
              exit 1;
          fi
      fi

.deploy_template: &deploy_definition
  image: {{ gitlab_builder_image }}
  stage: deploy
  retry: 2
  <<: *tags_definition
  script:
    - for tag in $(env | grep 'CUDA.*_TAGS' | cut -f2 -d=); do
        {# [[ $tag == "\"\"" ]] && continue; #}
        echo ${tag//\"} >> TAG_MANIFEST;
      done
    - cat TAG_MANIFEST
    - if [ ! -z $KITMAKER ] && [ ! -z $TRIGGER ]; then
        retry 3 10 time wget --directory-prefix="kitpick" --reject="index.html*" --no-parent ${CANDIDATE_URL}/manifest-${OS_NAME}-${OS_VERSION}.yml;
        export MANIFEST="kitpick/manifest-${OS_NAME}-${OS_VERSION}.yml";
      fi
    - python manager.py ${MANIFEST:+`echo "--manifest ${MANIFEST}"`} push
        --tag-manifest TAG_MANIFEST
        --image-name "${IMAGE_NAME}"
        --os-name "${OS_NAME}"
        --os-version "${OS_VERSION}"
        --cuda-version "${CUDA_VERSION}"
        --arch "${ARCH}"
        ${PIPELINE_NAME:+`echo "--pipeline-name ${PIPELINE_NAME}"`}
        ${IMAGE_TAG_SUFFIX:+`echo "--tag-suffix ${IMAGE_TAG_SUFFIX}"`} ${NO_PUSH:+"-n"}
    - |
      cat $MANIFEST | grep "${ARCH}.*urm.nvidia.com" | cut -f2 -d: | xargs >> TAG_MANIFEST
  artifacts:
    paths:
      - TAG_MANIFEST

trigger:
  image: {{ gitlab_builder_image }}
  stage: trigger
  <<: *tags_definition
  variables:
    MANIFEST: "{{ cuda.manifest_path }}"
  script:
    - echo CI_COMMIT_MESSAGE:$CI_COMMIT_MESSAGE
    - export ESC_TO=`echo $TRIGGER_OVERRIDE | sed 's/\ //g'`
    - export CMD="python manager.py ${MANIFEST:+`echo --manifest ${MANIFEST}`} trigger ${TRIGGER_OVERRIDE:+`echo --trigger-override ${ESC_TO}`}"
    - 'echo "COMMAND: $CMD"'
    - $CMD
  rules:
    {# - if: '$TRIGGER == null' #}
    - if: '$TRIGGER_OVERRIDE'

# Need to pass these from the trigger
.kitmaker_variables: &kitmaker_variables
  IMAGE_NAME: "gitlab-master.nvidia.com:5005/cuda-installer/cuda/release-candidate/${ARCH}"

.kitmaker_only: &kitmaker_only
  variables:
    <<: *kitmaker_variables
  rules:
    - if: '$TRIGGER == "true" && $KITMAKER == "true"'
      when: on_success

kitmaker_base:
  <<: *cuda_base_definition
  <<: *kitmaker_only

kitmaker_deploy:
  <<: *deploy_definition
  <<: *kitmaker_only
  needs:
    - job: kitmaker_base
      artifacts: true

# Requires two stages because CI_JOB_STATUS is not yet available in our infrastructure
kitmaker_cleanup_success:
  image: {{ gitlab_builder_image }}
  stage: cleanup_success
  <<: *tags_definition
  script:
    - |
      cat TAG_MANIFEST
      export a_image_name=$(awk '/./{line=$0} END{print line}' TAG_MANIFEST) # get the artifactory repo name
      sed -i '$ d' TAG_MANIFEST # delete the last line containing the artifactory repo
      export json_data="{\"status\": \"success\", \"gitlab_pipeline_url\": \"${CI_PIPELINE_URL}\", \"image_name\": \"${a_image_name}\", \"tags\": $(cat TAG_MANIFEST | jq -R . | jq -s . | jq 'map(select(length > 0))' | jq -c .)}"
      curl -v -X POST -H "Content-Type: application/json" -d "${json_data}" "${WEBHOOK_URL}"
  variables:
    <<: *kitmaker_variables
  rules:
    - if: '$TRIGGER == "true" && $KITMAKER == "true"'
      when: on_success
  needs:
    - job: kitmaker_base
      artifacts: true
    - job: kitmaker_deploy

# Requires two stages because CI_JOB_STATUS is not yet available in our infrastructure
kitmaker_cleanup_failed:
  image: {{ gitlab_builder_image }}
  stage: cleanup_failed
  <<: *tags_definition
  script:
    - |
      export json_data="{\"status\": \"failed\", \"gitlab_pipeline_url\": \"${CI_PIPELINE_URL}\"}"
      curl -v -X POST -H "Content-Type: application/json" -d "${json_data}" ${WEBHOOK_URL}
  variables:
    <<: *kitmaker_variables
  rules:
    - if: '$TRIGGER == "true" && $KITMAKER == "true"'
      when: on_failure
  needs:
    - job: kitmaker_base
    - job: kitmaker_deploy
      artifacts: true

{% for cuda_version in cuda if not cuda_version.startswith("manifest") %}
{% for pipeline in cuda[cuda_version] %}
{% for distro in cuda[cuda_version][pipeline] %}
{% for arch in cuda[cuda_version][pipeline][distro]["arches"] -%}

{% set cuda_version_yaml_safe = cuda[cuda_version][pipeline]["yaml_safe"] %}
{% set distro_yaml_safe = cuda[cuda_version][pipeline][distro]["yaml_safe"] -%}
{% set image_tag_suffix = cuda[cuda_version][pipeline][distro]["image_tag_suffix"] -%}
{% set pipeline_name_us = "_" + pipeline if pipeline != "default" else "" -%}
{% set pipeline_name_hyp = "-" + pipeline if pipeline != "default" else "" -%}

.{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_{{ arch }}_variables: &{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_{{ arch }}_variables
  ARCH: "{{ arch }}"
  DIST_BASE_PATH: "{{ cuda[cuda_version][pipeline]["dist_base_path"] }}"
  IMAGE_NAME: "{{ cuda[cuda_version][pipeline][distro]["image_name"][arch] }}"
  MANIFEST: "{{ cuda.manifest_path }}"
  {% if image_tag_suffix %}
  IMAGE_TAG_SUFFIX: {{ image_tag_suffix }}
  {% endif %}
  {% if cuda[cuda_version][pipeline][distro]["no_os_suffix"][arch] %}
  NO_OS_SUFFIX: "true"
  {% endif %}
  OS: "{{ distro }}"
  OS_NAME: "{{ cuda[cuda_version][pipeline][distro]["name"] }}"
  OS_VERSION: "{{ cuda[cuda_version][pipeline][distro]["version"] }}"
  CUDA_VERSION: "{{ cuda_version }}"
  {% if pipeline != "default" %}
  PIPELINE_NAME: "{{ pipeline }}"
  {% endif %}

.{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_{{ arch }}_only: &{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_{{ arch }}_only
{% if arch == "x86_64" %}
  <<: *tags_definition
{% else %}
  <<: *tags_definition_multiarch
{% endif %}
  variables:
    <<: *{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_{{ arch }}_variables
  rules:
    - if: '${{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_{{ arch }} == "true"'
    - if: '$all == "true"'

{{ distro }}-v{{ cuda_version }}{{ pipeline_name_hyp }}-{{ arch }}:
{% if cuda_version == "8.0" %}
  <<: *cuda_definition_deprecated
{% else %}
  <<: *cuda_base_definition
{% endif %}
  <<: *{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_{{ arch }}_only

{% set cudnn_stages = [] %}
{% if cuda[cuda_version][pipeline][distro]["cudnn"] %}
{% for cudnn in cuda[cuda_version][pipeline][distro]["cudnn"][arch] -%}

{{ distro }}-v{{ cuda_version }}{{ pipeline_name_hyp }}-{{ cudnn }}-{{ arch }}:
  <<: *cudnn_definition
  <<: *{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_{{ arch }}_only
  # Variables overwritten here. Don't move this section.
  variables:
    <<: *{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_{{ arch }}_variables
    CUDNN_VERSION: "{{ cudnn }}"
  artifacts:
    reports:
      dotenv: {{ cudnn }}finaltags.env
  needs:
    - job: {{ distro }}-v{{ cuda_version }}{{ pipeline_name_hyp }}-{{ arch }}
      artifacts: true

{% if pipeline and pipeline != "default" %}
  {% do cudnn_stages.append(distro + "-v" + cuda_version + "-" + pipeline + "-" + cudnn + "-" + arch) %}
{% else %}
  {% do cudnn_stages.append(distro + "-v" + cuda_version + "-" + cudnn + "-" + arch) %}
{% endif %}
{% endfor %}
{% endif -%}

{{ distro }}-v{{ cuda_version }}{{ pipeline_name_hyp }}-test-{{ arch }}:
  needs: ["{{ distro }}-v{{ cuda_version }}{{ pipeline_name_hyp }}-{{ arch }}"]
  <<: *{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_{{ arch }}_only
  <<: *test_definition
  rules:
    - if: '$NO_TEST == "true"'
      when: never
    - if: '${{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_{{ arch }} == "true"'
    - if: '$all == "true"'

{{ distro }}-v{{ cuda_version }}{{ pipeline_name_hyp }}-scan-{{ arch }}:
  <<: *scan_definition
  <<: *{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_{{ arch }}_only
  needs:
    - job: {{ distro }}-v{{ cuda_version }}{{ pipeline_name_hyp }}-{{ arch }}
    - job: {{ distro }}-v{{ cuda_version }}{{ pipeline_name_hyp }}-test-{{ arch }}
  # Override rules from only definition
  rules:
    - if: '$NO_SCAN == "true" || $NO_TEST == "true"'
      when: never
    - if: '${{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_{{ arch }} == "true"'
    - if: '$all == "true"'

{{ distro }}-v{{ cuda_version }}{{ pipeline_name_hyp }}-scan-no-test-{{ arch }}:
  <<: *scan_definition
  <<: *{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_{{ arch }}_only
  # Override rules from only definition
  needs:
    - job: {{ distro }}-v{{ cuda_version }}{{ pipeline_name_hyp }}-{{ arch }}
  rules:
    - if: '$NO_SCAN == "true"'
      when: never
    - if: '${{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_{{ arch }} == "true" && $NO_TEST == "true"'

{% set needs = [
    "{}-v{}{}-{}".format(distro, cuda_version, pipeline_name_hyp, arch),
    "{}-v{}{}-test-{}".format(distro, cuda_version, pipeline_name_hyp, arch),
    "{}-v{}{}-scan-{}".format(distro, cuda_version, pipeline_name_hyp, arch),
    "{}-v{}{}-scan-no-test-{}".format(distro, cuda_version, pipeline_name_hyp, arch),
] %}
{% if cudnn_stages %}
    {# {% set needs = [] %} #}
    {% for cudnn_stage in cudnn_stages %}
        {% do needs.append(cudnn_stage) %}
    {% endfor %}
{% endif -%}

{{ distro }}-v{{ cuda_version }}{{ pipeline_name_hyp }}-deploy-{{ arch }}:
  <<: *deploy_definition
  <<: *{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_{{ arch }}_only
  # Override rules from only definition
  rules:
    - if: '$NO_TEST == "true" || $NO_SCAN == "true"'
      when: never
    - if: '${{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_{{ arch }} == "true"'
    - if: '$all == "true"'
  needs:
  {# This silly workaround is needed because variables lose scope in a for loop #}
  {% set stahp = { 'flag': True } %}
  {% for job in needs %}
    {% if "-scan-no-test-" in job %}
    {% else %}
    - job: {{ job }}
      {% if stahp.flag %}
      artifacts: true
        {% do stahp.update({'flag':False}) %}
      {% endif %}
    {% endif %}
  {% endfor %}

# workaround for gitlab ci rules when NO_TEST and NO_SCAN are used...
{{ distro }}-v{{ cuda_version }}{{ pipeline_name_hyp }}-deploy-{{ arch }}-no-test-no-scan:
  <<: *deploy_definition
  <<: *{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_{{ arch }}_only
  rules:
    - if: '${{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_{{ arch }} == "true" && $NO_TEST == "true" && $NO_SCAN == "true"'
  needs:
  {# This silly workaround is needed because variables lose scope in a for loop #}
  {% set stahp = { 'flag': True } %}
  {% for job in needs %}
    {% if "-test-" in job %}
    {% elif "-scan-" + arch in job %}
    {% else %}
    - job: {{ job }}
      {% if stahp.flag %}
      artifacts: true
        {% do stahp.update({'flag':False}) %}
      {% endif %}
    {% endif %}
  {% endfor %}

# workaround for gitlab ci rules when NO_TEST is used...
{{ distro }}-v{{ cuda_version }}{{ pipeline_name_hyp }}-deploy-{{ arch }}-no-test:
  <<: *deploy_definition
  <<: *{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_{{ arch }}_only
  rules:
    - if: '$NO_SCAN == "true"'
      when: never
    - if: '${{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_{{ arch }} == "true" && $NO_TEST == "true"'
  needs:
  {# This silly workaround is needed because variables lose scope in a for loop #}
  {% set stahp = { 'flag': True } %}
  {% for job in needs %}
    {% if not "-scan-no-test-" in job and "-test-" in job %}
    {% elif not "-scan-no-test-" in job and "-scan-" in job %}
    {% else %}
    - job: {{ job }}
      {% if stahp.flag %}
      artifacts: true
        {% do stahp.update({'flag':False}) %}
      {% endif %}
    {% endif %}
  {% endfor %}

# workaround for gitlab ci rules when NO_SCAN is used...
{{ distro }}-v{{ cuda_version }}{{ pipeline_name_hyp }}-deploy-{{ arch }}-no-scan:
  <<: *deploy_definition
  <<: *{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_{{ arch }}_only
  rules:
    - if: '$NO_TEST == "true"'
      when: never
    - if: '${{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_{{ arch }} == "true" && $NO_SCAN == "true"'
  needs:
  {# This silly workaround is needed because variables lose scope in a for loop #}
  {% set stahp = { 'flag': True } %}
  {% for job in needs %}
    {% if "-scan-" in job %}
    {% else %}
    - job: {{ job }}
      {% if stahp.flag %}
      artifacts: true
        {% do stahp.update({'flag':False}) %}
      {% endif %}
    {% endif %}
  {% endfor %}

{% endfor -%}
{% endfor %}
{% endfor %}
{% endfor %}
